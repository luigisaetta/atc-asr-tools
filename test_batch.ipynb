{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd0d0e3",
   "metadata": {},
   "source": [
    "### Test ASR model in batch mode\n",
    "\n",
    "* consider all the wav files contained in a directory\n",
    "* two modes: 1. requires a csv file with path and sentence (can be empty), 2. process all wav in a dir\n",
    "* check that all the files have the required characteristics (sample_rate=16000)\n",
    "* apply the model and makes a prediction for each file\n",
    "* eventually compare prediction with expected and compute WER\n",
    "* apply SpellChecker\n",
    "* compute WER after Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a03000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from datasets import Dataset\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import soundfile as sf\n",
    "import re\n",
    "\n",
    "# for model inference\n",
    "import json\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "import torch\n",
    "\n",
    "# used to compute WER\n",
    "from jiwer import wer\n",
    "\n",
    "from utils import check_sample_rate, init_empty_list, check_files_exists, check_gpu\n",
    "from utils import check_mono\n",
    "\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b681f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "\n",
    "# a DIR containing a csv file with path,sentence\n",
    "# a list of WAV files one for each rows in csv\n",
    "# wav files are MONO, sample_rate = 16 Khz\n",
    "\n",
    "# if it is set to True it expects a csv file with the list and paths of the wav. Otherwise\n",
    "# it process all the wav files in the DIR_4_TEST\n",
    "CSV_MODE = True\n",
    "# save all predictions in csv file\n",
    "CREATE_OUT_CSV = True\n",
    "\n",
    "DIR_4_TEST = \"/home/datascience/asr-atc/data2_4_test/\"\n",
    "CSV_FILE_NAME = \"test.csv\"\n",
    "# DIR_4_TEST = \"/home/datascience/asr-atc/data_4_test_train/\"\n",
    "# CSV_FILE_NAME = \"atco2.csv\"\n",
    "\n",
    "# the output file with all predictions\n",
    "OUT_CSV = \"predictions.csv\"\n",
    "\n",
    "# the directory containing the files of the trained model\n",
    "REPO_NAME = \"wav2vec2-large-xls-r-300m-tr-ls\"\n",
    "VOCAB_DIR = \"./vocab_atco2\"\n",
    "\n",
    "# globals\n",
    "# 16 Khz\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c865b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available, OK\n"
     ]
    }
   ],
   "source": [
    "# It is expected that this notebook runs on GPU (otherwise, remove tocuda)\n",
    "# check it\n",
    "check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b736870",
   "metadata": {},
   "source": [
    "#### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bed45f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded !!!\n"
     ]
    }
   ],
   "source": [
    "# load everything\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(VOCAB_DIR, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=SAMPLE_RATE, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# it is expected to run on GPU (to(cuda))\n",
    "model = Wav2Vec2ForCTC.from_pretrained(REPO_NAME).to(\"cuda\")\n",
    "\n",
    "print()\n",
    "print(\"Model loaded !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "504dbf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Functions\n",
    "#\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "def do_test(index):\n",
    "    # from input to prediction\n",
    "    input_dict = processor(ds_hf_test[index][\"input_values\"], return_tensors=\"pt\", padding=True, sampling_rate=SAMPLE_RATE)\n",
    "    # it is expected to run on GPU (to(cuda))\n",
    "    logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "    pred_text = processor.decode(pred_ids)\n",
    "    \n",
    "    print()\n",
    "    print(f\"Prediction on: {list_wav[index]}\")\n",
    "    print(pred_text)\n",
    "    \n",
    "    if CSV_MODE == True:\n",
    "        print()\n",
    "        print(\"Expected:\")\n",
    "        print(list_txts[index].lower())\n",
    "    print()\n",
    "    \n",
    "    return pred_text\n",
    "\n",
    "# do test in batch mode da problemi con OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89e762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files list from csv or all wav from directory\n",
    "\n",
    "if CSV_MODE == True:\n",
    "    # there is a csv file that will guide\n",
    "    CSV_FULL_NAME = DIR_4_TEST + CSV_FILE_NAME\n",
    "\n",
    "    df_test = pd.read_csv(CSV_FULL_NAME)\n",
    "\n",
    "    df_test.head()\n",
    "    \n",
    "    # create the list of wav from DataFrame\n",
    "    list_wav = list(df_test['path'].values)\n",
    "else:\n",
    "    # build the list of wav directly from the contents of directory\n",
    "    print(f\"Not using CSV file ...reading list of wav from directory {DIR_4_TEST}\")\n",
    "    \n",
    "    list_wav = sorted(glob.glob(DIR_4_TEST + \"*.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c797c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All required wav files are available!\n"
     ]
    }
   ],
   "source": [
    "# check that all wav files are available\n",
    "check_files_exists(list_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89f37cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All wav files have sample rate = 16000.\n"
     ]
    }
   ],
   "source": [
    "# check that all files have SAMPLE_RATE\n",
    "check_sample_rate(list_wav, ref_sample_rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0bf4650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All wav files are MONO.\n"
     ]
    }
   ],
   "source": [
    "# check that all files are MONO\n",
    "check_mono(list_wav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1a5158",
   "metadata": {},
   "source": [
    "#### Create the HF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd497a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF dataset created !\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# load all data in HF dataset\n",
    "#\n",
    "list_path_names = list_wav\n",
    "\n",
    "if CSV_MODE == True:\n",
    "    list_txts = list(df_test['sentence'].values)\n",
    "else:\n",
    "    # no expected values available\n",
    "    list_txts = init_empty_list(len(list_wav))\n",
    "\n",
    "# create a dictionary\n",
    "dict_res = {\"path\": list_path_names, \"audio\" : list_path_names, \"sentence\": list_txts}\n",
    "\n",
    "# create the dataset in HF format\n",
    "ds_hf_test = Dataset.from_dict(dict_res).cast_column(\"audio\", Audio())\n",
    "\n",
    "print(\"HF dataset created !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ebde01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 9 records in the dataset to be used for test.\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {len(ds_hf_test)} records in the dataset to be used for test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "598b809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a final check for compatibility with HF example\n",
    "\n",
    "# get a random index to select a randmo item from the dataset\n",
    "# rand_int = random.randint(0, len(ds_hf_test)-1)\n",
    "\n",
    "# print()\n",
    "# print(f\"Checking record n. {rand_int}\")\n",
    "# print(\"Input audio array shape:\", ds_hf_test[rand_int][\"audio\"][\"array\"].shape)\n",
    "# print(\"Sampling rate:\", ds_hf_test[rand_int][\"audio\"][\"sampling_rate\"])\n",
    "\n",
    "# if CSV_MODE == True:\n",
    "#    print(\"Expected text:\", ds_hf_test[rand_int][\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0472276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for inference....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc82d096f7bd47d4ba86ab76c4389a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the dataset for inference\n",
    "\n",
    "print(\"Preparing dataset for inference....\")\n",
    "\n",
    "ds_hf_test = ds_hf_test.map(prepare_dataset, remove_columns=ds_hf_test.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba1595",
   "metadata": {},
   "source": [
    "#### Ready: now we have all files packed in a HF dataset ready to be used for test\n",
    "#### Do Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "718f3f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction on: /home/datascience/asr-atc/data2_4_test/luigi1.wav\n",
      "alpha bata gma delta\n",
      "\n",
      "Expected:\n",
      "alfa beta gamma delta\n",
      "\n",
      "\n",
      "Prediction on: /home/datascience/asr-atc/data2_4_test/luigi2.wav\n",
      "euro wind seven alpha bravo turnd right heading two one zero cleared ils approach runway two four leport established\n",
      "\n",
      "Expected:\n",
      "eurowings seven alfa bravo turn right heading two one zero cleared ils approach runway two four report established\n",
      "\n",
      "\n",
      "Prediction on: /home/datascience/asr-atc/data2_4_test/luigi3.wav\n",
      "rya aunawr seven three halpho tol turn left heading three six zero\n",
      "\n",
      "Expected:\n",
      "ryanair seven three alpha hotel turn left heading three six zero\n",
      "\n",
      "\n",
      "Prediction on: /home/datascience/asr-atc/data2_4_test/luigi4.wav\n",
      "rya yai noawr seven three allpha hotoel\n",
      "\n",
      "Expected:\n",
      "ryanair seven three alpha hotel\n",
      "\n",
      "\n",
      "Prediction on: /home/datascience/asr-atc/data2_4_test/luigi5.wav\n",
      "oscar kilo kilo uniform november proceed direct lybiltu\n",
      "\n",
      "Expected:\n",
      "oscar kilo kilo uniform november proceed direct baltu\n",
      "\n",
      "\n",
      "Prediction on: /home/datascience/asr-atc/data2_4_test/luigi6.wav\n",
      "prosedend e reict maltu ooscar kilo kilo uniform november\n",
      "\n",
      "Expected:\n",
      "proceed direct baltu oscar kilo kilo uniform november\n",
      "\n",
      "\n",
      "Prediction on: /home/datascience/asr-atc/data2_4_test/luigi8.wav\n",
      "htel delta lima runway vacatet on delta\n",
      "\n",
      "Expected:\n",
      "hotel delta lima runway vacated on delta\n",
      "\n",
      "\n",
      "Prediction on: /home/datascience/asr-atc/data2_4_test/luigi9.wav\n",
      "bluparking hotel delta lima\n",
      "\n",
      "Expected:\n",
      "blue parking hotel delta lima\n",
      "\n",
      "\n",
      "Prediction on: /home/datascience/asr-atc/data2_4_test/luigi10.wav\n",
      "hotel bravo charlie lima hotel v gho line up and wait runway one zero\n",
      "\n",
      "Expected:\n",
      "hotel bravo charlie lima hotel via echo line up and wait runway one zero\n",
      "\n",
      "CPU times: user 13.3 s, sys: 1.66 s, total: 14.9 s\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if CREATE_OUT_CSV == True:\n",
    "    list_predictions = []\n",
    "    \n",
    "for INDEX in range(len(ds_hf_test)):\n",
    "    str_pred = do_test(INDEX)\n",
    "    \n",
    "    if CREATE_OUT_CSV == True:\n",
    "        list_predictions.append(str_pred)\n",
    "\n",
    "if CREATE_OUT_CSV == True:\n",
    "    # create the output csv file\n",
    "    out_dict = {\"file\":list_wav, \"preds\":list_predictions}\n",
    "    \n",
    "    out_df = pd.DataFrame.from_dict(out_dict)\n",
    "    \n",
    "    out_df.to_csv(DIR_4_TEST + OUT_CSV, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af384b",
   "metadata": {},
   "source": [
    "#### Compute WER on all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abced7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computed WER is: 0.362\n"
     ]
    }
   ],
   "source": [
    "# compute WER\n",
    "\n",
    "# all to lower case to compute wer\n",
    "list_txts = [txt.lower() for txt in list_txts]\n",
    "\n",
    "if (CSV_MODE == True) and (CREATE_OUT_CSV == True):\n",
    "    v_wer = wer(list_txts, list_predictions)\n",
    "    \n",
    "    print()\n",
    "    print(f\"Computed WER is: {round(v_wer, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46a5b9",
   "metadata": {},
   "source": [
    "### Adding Spell Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c84f551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bcee4971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#\n",
    "# Incapsulo correct_txt in ATCSpellChecker\n",
    "#\n",
    "class ATCSpellChecker(SpellChecker):\n",
    "    def __init__(self):\n",
    "        SpellChecker.__init__(self)\n",
    "        # carico il testo atco2 per integrare le parole \"speciali\" nel dizionario\n",
    "        self.word_frequency.load_text_file('./atco2.txt')\n",
    "    \n",
    "    def correct_text(self, text):\n",
    "        # To correct an entire sentence\n",
    "        SEP = \" \"\n",
    "        l_text = self.split_words(text)\n",
    "    \n",
    "        l_text_corrected = [self.correction(w) if self.correction(w) is not None else w for w in l_text]\n",
    "        \n",
    "        # rebuild the sentence and return\n",
    "        return SEP.join(l_text_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3e0d72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = ATCSpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "207c1582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha data ma delta\n",
      "euro wind seven alpha bravo turn right heading two one zero cleared ils approach runway two four report established\n",
      "ya lunar seven three alpha to turn left heading three six zero\n",
      "ya yai now seven three alpha hotel\n",
      "oscar kilo kilo uniform november proceed direct lybiltu\n",
      "prosedend e react malt oscar kilo kilo uniform november\n",
      "hotel delta lima runway vacate on delta\n",
      "bluparking hotel delta lima\n",
      "hotel bravo charlie lima hotel i go line up and wait runway one zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_corr_predictions = [spell.correct_text(text) for text in tqdm(list_predictions)]\n",
    "\n",
    "for corr_txt in list_corr_predictions:\n",
    "    print(corr_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8b6cc597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computed WER after spell corrections is: 0.275\n"
     ]
    }
   ],
   "source": [
    "v_wer = wer(list_txts, list_corr_predictions)\n",
    "    \n",
    "print()\n",
    "print(f\"Computed WER after spell corrections is: {round(v_wer, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2f1fa9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lybiltu'}\n",
      "{'prosedend'}\n",
      "{'bluparking'}\n"
     ]
    }
   ],
   "source": [
    "# let's see if we identify unknown words\n",
    "for corr_txt in list_corr_predictions:\n",
    "    l_text = spell.split_words(corr_txt)\n",
    "    \n",
    "    unk = spell.unknown(l_text)\n",
    "    if len(unk) > 0:\n",
    "        print(unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0c5ad675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baltu'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.known([\"baltu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5bcbba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch110_p37_gpu_v1]",
   "language": "python",
   "name": "conda-env-pytorch110_p37_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
